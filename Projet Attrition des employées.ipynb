{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d95a5f",
   "metadata": {},
   "source": [
    "## Projet : Employee-Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f39fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier,DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import stddev, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076d0f4",
   "metadata": {},
   "source": [
    "## chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e788605",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('attrition').getOrCreate()\n",
    "file_location = \"HR-Employee-Attrition.csv\"\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".option(\"header\", first_row_is_header) \\\n",
    ".option(\"sep\", delimiter) \\\n",
    ".load(file_location)\n",
    "\n",
    "df = df.drop(\"Over18\")\n",
    "df = df.drop(\"EmployeeCount\")\n",
    "df = df.drop(\"StandardHours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66457905",
   "metadata": {},
   "source": [
    "## grid pour les modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4acb80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol='features', labelCol='label')\n",
    "param_grid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2, 4, 6]) \\\n",
    "    .addGrid(gbt.maxBins, [20, 30]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2]) \\\n",
    "    .addGrid(rf.maxBins, [20]) \\\n",
    "    .addGrid(rf.numTrees, [10]) \\\n",
    "    .build()\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "param_grid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.05, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea59459",
   "metadata": {},
   "source": [
    "## fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f858d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(df):\n",
    "    # convertir les colonnes string en double quand cela est possible\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            if int(df.select(column).first()[0]):\n",
    "                df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "        except:\n",
    "            pass\n",
    "    return df\n",
    "        \n",
    "def remove_outliers(df):\n",
    "\n",
    "    # Boucle pour enlever les valeurs aberrantes de chaque colonne\n",
    "    for col in df.columns:\n",
    "        # Calcul des statistiques pour la colonne\n",
    "        stats = df.select(avg(col), stddev(col)).first()\n",
    "        mean = stats[0]\n",
    "        std = stats[1]  \n",
    "        if mean is not None and std is not None:\n",
    "            # Calcul du seuil pour déterminer les valeurs aberrantes\n",
    "            threshold = 3 * std + mean     \n",
    "            # Suppression des valeurs aberrantes pour la colonne\n",
    "            df = df.filter(df[col] <= threshold)\n",
    "    return df\n",
    "\n",
    "\n",
    "def assembler_for_pipeline(df, label):\n",
    "    categoricalColumns = [col for (col, dtype) in df.dtypes if dtype == \"string\" and col != label]\n",
    "    stages = []\n",
    "\n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "    \n",
    "    label_stringIdx = StringIndexer(inputCol=label, outputCol=\"label\")\n",
    "    stages += [label_stringIdx]\n",
    "\n",
    "    numericCols = [col for (col, dtype) in df.dtypes if dtype.startswith('double') and col != \"label\"]\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    return assembler, stages\n",
    "\n",
    "def model_for_pipeline(classifier, param_grid):\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "    cv = CrossValidator(estimator=classifier, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "    return cv\n",
    "\n",
    "def view_feature_importances(df):\n",
    "    \n",
    "    assembler, stages = assembler_for_pipeline(df, 'Attrition')\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 20]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .build()\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    train, test = df.randomSplit([0.8, 0.2])\n",
    "    stages += [assembler, rf]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    pipelineModel = pipeline.fit(train)\n",
    "    predictions = pipelineModel.transform(test)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(\"AUC-ROC = %g\" % auc)\n",
    "    \n",
    "    importances = pipelineModel.stages[-1].featureImportances.toArray()\n",
    "    cols = df.columns\n",
    "    selectedcols = [\"label\", \"features\"] + cols\n",
    "    cols_importances = list(zip(selectedcols, importances))\n",
    "    sorted_cols_importances = sorted(cols_importances, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Colonnes triées par ordre d'importance décroissante :\")\n",
    "    for col, importance in sorted_cols_importances:\n",
    "        print(col, \"=\", importance)\n",
    "        \n",
    "    return sorted_cols_importances\n",
    "        \n",
    "def pipelinedata(df, assembler, stages, model):\n",
    "    \n",
    "    stages += [assembler, model]\n",
    "    train, test = df.randomSplit([0.8, 0.2])\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(train)\n",
    "    predictions = pipelineModel.transform(test)\n",
    "    predictions.take(1)\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\")\n",
    "    display(selected)\n",
    "    return model, predictions, selected\n",
    "\n",
    "def metrics(predictions, model):\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    auc_roc = evaluator.evaluate(predictions)\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(model + \": courbe ROC = %g\" % auc_roc)\n",
    "    print(model + \": précision = %g\" % accuracy)\n",
    "    print(model + \": RMSE = %g\" % rmse)\n",
    "    \n",
    "def filter_most_important_columns(df, sorted_cols_importances):\n",
    "    selected_cols = [col for col, importance in sorted_cols_importances if importance > 0.01]\n",
    "    if \"features\" in selected_cols:\n",
    "        selected_cols.remove(\"features\")\n",
    "    if \"label\" in selected_cols:\n",
    "        selected_cols.remove(\"label\")\n",
    "    df = df.select(selected_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ea14a",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f26880ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_to_int(df)\n",
    "df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f99576",
   "metadata": {},
   "source": [
    "## Analyse de l'importance des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b08392c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC = 0.724248\n",
      "Colonnes triées par ordre d'importance décroissante :\n",
      "MonthlyRate = 0.13685865373538764\n",
      "NumCompaniesWorked = 0.032945466495976\n",
      "MonthlyIncome = 0.025491215407781563\n",
      "OverTime = 0.018201824137640574\n",
      "features = 0.016385920305205134\n",
      "JobLevel = 0.013893615360948667\n",
      "Attrition = 0.012996559215915976\n",
      "YearsWithCurrManager = 0.008102447779980333\n",
      "DistanceFromHome = 0.005671837218922965\n",
      "WorkLifeBalance = 0.005042907983606777\n",
      "PerformanceRating = 0.004990846427949578\n",
      "YearsInCurrentRole = 0.0048987232040918465\n",
      "BusinessTravel = 0.004644320539659293\n",
      "MaritalStatus = 0.004205692609634758\n",
      "Education = 0.0035786602838141874\n",
      "YearsSinceLastPromotion = 0.0035647627074464443\n",
      "TotalWorkingYears = 0.0034642039284802574\n",
      "Department = 0.0034251160246222844\n",
      "Gender = 0.00335503866629112\n",
      "Age = 0.0029518212842760947\n",
      "EducationField = 0.002116198314833574\n",
      "DailyRate = 0.001938849577271772\n",
      "JobSatisfaction = 0.0018806938462507493\n",
      "label = 0.0018618002247707609\n",
      "HourlyRate = 0.001236978023191154\n",
      "YearsAtCompany = 0.0009687501016412845\n",
      "RelationshipSatisfaction = 0.0007325833425570756\n",
      "EnvironmentSatisfaction = 0.0007162366663839339\n",
      "EmployeeNumber = 0.0003818685688393052\n",
      "StockOptionLevel = 0.00022627455845783826\n",
      "JobInvolvement = 0.0\n",
      "JobRole = 0.0\n",
      "PercentSalaryHike = 0.0\n",
      "TrainingTimesLastYear = 0.0\n"
     ]
    }
   ],
   "source": [
    "sorted_cols_importances = view_feature_importances(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f23b3",
   "metadata": {},
   "source": [
    "## Garder les colonnes les plus importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b6c6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_most_important_columns(df, sorted_cols_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b0e8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------+--------+--------+---------+\n",
      "|MonthlyRate|NumCompaniesWorked|MonthlyIncome|OverTime|JobLevel|Attrition|\n",
      "+-----------+------------------+-------------+--------+--------+---------+\n",
      "|    19479.0|               8.0|       5993.0|     Yes|     2.0|      Yes|\n",
      "|    24907.0|               1.0|       5130.0|      No|     2.0|       No|\n",
      "|     2396.0|               6.0|       2090.0|     Yes|     1.0|      Yes|\n",
      "|    23159.0|               1.0|       2909.0|     Yes|     1.0|       No|\n",
      "|    16632.0|               9.0|       3468.0|      No|     1.0|       No|\n",
      "|    11864.0|               0.0|       3068.0|      No|     1.0|       No|\n",
      "|     9964.0|               4.0|       2670.0|     Yes|     1.0|       No|\n",
      "|    13335.0|               1.0|       2693.0|      No|     1.0|       No|\n",
      "|     8787.0|               0.0|       9526.0|      No|     3.0|       No|\n",
      "|    16577.0|               6.0|       5237.0|      No|     2.0|       No|\n",
      "+-----------+------------------+-------------+--------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c162948",
   "metadata": {},
   "source": [
    "## Modelisation avec les champs filtrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "157d47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbt = df\n",
    "df_rf = df\n",
    "df_lr = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fdbbf",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "effa289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = convert_to_int(df_rf)\n",
    "df_rf = remove_outliers(df_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b14a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler, stages = assembler_for_pipeline(df_rf, 'Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fe11607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, rawPrediction: vector, probability: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: courbe ROC = 0.743796\n",
      "rf: précision = 0.843137\n",
      "rf: RMSE = 0.396059\n"
     ]
    }
   ],
   "source": [
    "model_rf = model_for_pipeline(rf, param_grid_rf)\n",
    "model_rf, predictions_rf, selected_rf = pipelinedata(df_rf, assembler, stages, model_rf)\n",
    "metrics(predictions_rf, 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da452d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_c4df840f5f10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a4c20",
   "metadata": {},
   "source": [
    "### gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "640e84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbt = convert_to_int(df_gbt)\n",
    "df_gbt = remove_outliers(df_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c66202e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbt = convert_to_int(df_gbt)\n",
    "df_gbt = remove_outliers(df_gbt)\n",
    "\n",
    "assembler, stages = assembler_for_pipeline(df_gbt, 'Attrition')\n",
    "\n",
    "model_gbt = model_for_pipeline(gbt, param_grid_gbt)\n",
    "model_gbt, predictions_gbt, selected_gbt = pipelinedata(df_gbt, assembler, stages, model_gbt)\n",
    "metrics(predictions_gbt, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7977eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, rawPrediction: vector, probability: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbt: courbe ROC = 0.795025\n",
      "gbt: précision = 0.828685\n",
      "gbt: RMSE = 0.413902\n"
     ]
    }
   ],
   "source": [
    "model_gbt = model_for_pipeline(gbt, param_grid_gbt)\n",
    "model_gbt, predictions_gbt, selected_gbt = pipelinedata(df_gbt, assembler, stages, model_gbt)\n",
    "metrics(predictions_gbt, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929991ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_gbt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31572/2018638763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_gbt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_gbt' is not defined"
     ]
    }
   ],
   "source": [
    "model_gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52492225",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a1add3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = convert_to_int(df_lr)\n",
    "df_lr = remove_outliers(df_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0e1099a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler, stages = assembler_for_pipeline(df_lr, 'Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a8d6b0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, rawPrediction: vector, probability: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: courbe ROC = 0.76598\n",
      "lr: précision = 0.818898\n",
      "lr: RMSE = 0.425561\n"
     ]
    }
   ],
   "source": [
    "model_lr = model_for_pipeline(lr, param_grid_lr)\n",
    "model_lr, predictions_lr, selected_lr = pipelinedata(df_lr, assembler, stages, model_lr)\n",
    "metrics(predictions_lr, 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c011bb8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31572/1823429032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_lr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_lr' is not defined"
     ]
    }
   ],
   "source": [
    "model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa63631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab45bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0bb95c",
   "metadata": {},
   "source": [
    "### j'ai pas utilisé cette partie, je te laisse modifié si besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3e55402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #paramétres\n",
    "# def params(nom, nb_model, train, test):\n",
    "#     liste = range(1, 20)\n",
    "\n",
    "#     best_accuracy = 0\n",
    "#     best_nb = 0\n",
    "\n",
    "#     for nb in liste:\n",
    "#         model_choix = [LogisticRegression(labelCol = 'label', featuresCol = 'features', maxIter= nb),\n",
    "#         DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=nb, seed =40),\n",
    "#         GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=nb, seed =40),\n",
    "#         RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=nb, seed =40)]\n",
    "\n",
    "#         model = model_choix[nb_model]\n",
    "#         trained_model = model.fit(train)\n",
    "#         predictions = trained_model.transform(test)\n",
    "#         evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "#         accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "#         # meilleure précision\n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_nb = nb\n",
    "        \n",
    "#     print(\"la meilleure précision pour \" + nom + \" :\", best_accuracy)\n",
    "#     print(\"le meilleur nombre pour paramétre \" + nom + \" :\", best_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb832a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_c4df840f5f10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0207ef1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8794.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.tuning.ValidatorParams$.saveImpl(ValidatorParams.scala:162)\r\n\tat org.apache.spark.ml.tuning.CrossValidator$CrossValidatorWriter.saveImpl(CrossValidator.scala:248)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31572/3242055157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a string, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"JavaMLWriter\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8794.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.tuning.ValidatorParams$.saveImpl(ValidatorParams.scala:162)\r\n\tat org.apache.spark.ml.tuning.CrossValidator$CrossValidatorWriter.saveImpl(CrossValidator.scala:248)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "model_rf.save(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('Model', 'zip','./content/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f07bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413cabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'rf': {'model': \"test1\", 'accuracy': 0.8181818181818182}, 'gbt': {'model': \"test2\", 'accuracy': 0.8284671532846716}, 'lr': {'model': \"test2\", 'accuracy': 0.8513011152416357}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ced874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is test2 with an accuracy of 0.8513011152416357.\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for model_name, model_info in my_dict.items():\n",
    "    accuracy = model_info['accuracy']\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model_info['model']\n",
    "\n",
    "print(f\"The best model is {best_model} with an accuracy of {best_accuracy}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57d3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
